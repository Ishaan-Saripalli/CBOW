import numpy as np 
import matplotlib.pyplot as plt 
import networkx as nx 
from sklearn.datasets import load_iris 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import OneHotEncoder, StandardScaler 
import random 
import math 

# Load IRIS dataset 
iris = load_iris() 
X = iris.data  # Features 
Y = iris.target.reshape(-1, 1)  # Labels 

# One-hot encoding of labels 
encoder = OneHotEncoder(sparse_output=False) 
Y_encoded = encoder.fit_transform(Y) 

# Normalize the features 
scaler = StandardScaler() 
X_scaled = scaler.fit_transform(X) 

# Split the dataset 
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_encoded, test_size=0.2, random_state=42) 

# Define neural network structure 
input_size = X.shape[1]  # 4 features 
hidden_size = 7  # Hidden layer with 7 neurons 
output_size = Y_encoded.shape[1]  # 3 classes 

# Initialize weights 
np.random.seed(42) 
w1 = np.random.randn(input_size, hidden_size) 
b1 = np.random.randn(hidden_size) 
w2 = np.random.randn(hidden_size, output_size) 
b2 = np.random.randn(output_size) 

# Activation function (sigmoid) 
def sigmoid(x): 
    return 1 / (1 + np.exp(-x)) 

def softmax(x): 
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)) 
    return exp_x / np.sum(exp_x, axis=1, keepdims=True) 

def forward_pass(X, w1, b1, w2, b2): 
    hidden_layer = sigmoid(np.dot(X, w1) + b1) 
    output_layer = softmax(np.dot(hidden_layer, w2) + b2) 
    return hidden_layer, output_layer 

def compute_loss(Y_pred, Y_true): 
    return -np.sum(Y_true * np.log(Y_pred + 1e-8)) / len(Y_true) 

# Simulated Annealing to optimize weights 
def simulated_annealing(X, Y, w1, b1, w2, b2, max_iter=1000, initial_temp=1.0, cooling_rate=0.995): 
    temp = initial_temp 
    current_w1, current_b1, current_w2, current_b2 = w1.copy(), b1.copy(), w2.copy(), b2.copy() 
    _, current_output = forward_pass(X, current_w1, current_b1, current_w2, current_b2) 
    current_loss = compute_loss(current_output, Y) 

    for i in range(max_iter): 
        # Generate new candidate weights 
        new_w1 = current_w1 + np.random.normal(0, 0.1, current_w1.shape) 
        new_b1 = current_b1 + np.random.normal(0, 0.1, current_b1.shape) 
        new_w2 = current_w2 + np.random.normal(0, 0.1, current_w2.shape) 
        new_b2 = current_b2 + np.random.normal(0, 0.1, current_b2.shape) 

        _, new_output = forward_pass(X, new_w1, new_b1, new_w2, new_b2) 
        new_loss = compute_loss(new_output, Y) 

        # Acceptance probability 
        if new_loss < current_loss or random.uniform(0, 1) < math.exp((current_loss - new_loss) / temp): 
            current_w1, current_b1, current_w2, current_b2 = new_w1, new_b1, new_w2, new_b2 
            current_loss = new_loss 

        # Reduce temperature 
        temp *= cooling_rate 

        if temp < 1e-8: 
            break 

    return current_w1, current_b1, current_w2, current_b2 

# Train the network using simulated annealing 
w1, b1, w2, b2 = simulated_annealing(X_train, Y_train, w1, b1, w2, b2) 

# Evaluate performance 
_, Y_pred = forward_pass(X_test, w1, b1, w2, b2) 
predictions = np.argmax(Y_pred, axis=1) 
y_true = np.argmax(Y_test, axis=1) 
accuracy = np.mean(predictions == y_true) 

print(f'Classification Accuracy: {accuracy * 100:.2f}%') 
